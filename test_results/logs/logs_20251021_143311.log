2025-10-21 14:33:11,247 - INFO - Args: Namespace(num_seeds=8, seeds=[774, 242, 744, 551], sequence_length=5, num_sequences=2, student_dims=[150, 200], eps_train=0.0001, w_that_minimizes_loss=False, w2_that_minimizes_loss=False, w2_that_maximizes_loss=False, data_file=None, gnc=True, gnc_num_samples=1000000000000, gnc_batch_size=10000000, gd=False, gd_lr=0.001, gd_epochs=10000, gd_init_scale=0.01, gd_optimizer='adam', gd_scheduler=None, gd_scheduler_params='{}', exp_gamma=None, step_size=None, step_gamma=None, cosine_eta_min=None, gd_init_type='regular', config=None, results_dir=PosixPath('test_results/results'), figures_dir=PosixPath('test_results/figures'), checkpoint_dir=PosixPath('test_results/checkpoints'), checkpoint_interval=3600, resume_from_checkpoint=False, log_dir=PosixPath('test_results/logs'), max_gpus=8, log_file=PosixPath('test_results/logs/logs_20251021_143311.log'))
2025-10-21 14:33:11,248 - INFO - Using custom seeds: [774, 242, 744, 551]
2025-10-21 14:33:11,455 - INFO - Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7]
2025-10-21 14:33:11,455 - INFO - Starting 8 processes on 8 GPUs
2025-10-21 14:33:15,564 - INFO - Process 0 started on GPU 0, processing seeds [774]
2025-10-21 14:33:16,061 - INFO - Process 3 started on GPU 3, processing seeds [551]
2025-10-21 14:33:16,070 - INFO - Process 1 started on GPU 1, processing seeds [242]
2025-10-21 14:33:16,101 - INFO - Process 2 started on GPU 2, processing seeds [744]
2025-10-21 14:33:16,527 - INFO - for seed 242, alpha_teacher=0.5, generated 2 sequences: [tensor([-0.2325,  1.6611, -2.0365, -0.1672], device='cuda:1'), tensor([ 1.2663,  1.6274, -1.7354, -0.5913], device='cuda:1')]
2025-10-21 14:33:16,528 - INFO - Process 1: Starting experiment - student_dim=150, seed=242
2025-10-21 14:33:16,608 - INFO - for seed 744, alpha_teacher=0.5, generated 2 sequences: [tensor([-0.0780,  0.0762, -0.8898,  1.5826], device='cuda:2'), tensor([-1.9488,  1.1827, -1.1090,  0.0646], device='cuda:2')]
2025-10-21 14:33:16,609 - INFO - Process 2: Starting experiment - student_dim=150, seed=744
2025-10-21 14:33:16,739 - INFO - for seed 551, alpha_teacher=0.5, generated 2 sequences: [tensor([ 0.7512, -0.7394,  1.7773,  0.4086], device='cuda:3'), tensor([-0.5732, -0.1409,  0.6480,  1.3442], device='cuda:3')]
2025-10-21 14:33:16,740 - INFO - Process 3: Starting experiment - student_dim=150, seed=551
2025-10-21 14:33:16,899 - INFO - for seed 774, alpha_teacher=0.5, generated 2 sequences: [tensor([-1.4639, -2.1893,  1.1826, -0.1660], device='cuda:0'), tensor([-0.4547,  1.9272, -0.2274,  0.0180], device='cuda:0')]
2025-10-21 14:33:16,900 - INFO - Process 0: Starting experiment - student_dim=150, seed=774
2025-10-21 14:33:17,206 - ERROR - G&C failed for student_dim=150, seed=774: CUDA out of memory. Tried to allocate 22.35 GiB. GPU 0 has a total capacity of 10.75 GiB of which 4.98 GiB is free. Including non-PyTorch memory, this process has 5.77 GiB memory in use. Of the allocated memory 5.59 GiB is allocated by PyTorch, and 3.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-21 14:33:17,206 - ERROR - G&C failed for student_dim=150, seed=744: CUDA out of memory. Tried to allocate 22.35 GiB. GPU 2 has a total capacity of 10.75 GiB of which 4.98 GiB is free. Including non-PyTorch memory, this process has 5.77 GiB memory in use. Of the allocated memory 5.59 GiB is allocated by PyTorch, and 3.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-21 14:33:17,206 - ERROR - G&C failed for student_dim=150, seed=242: CUDA out of memory. Tried to allocate 22.35 GiB. GPU 1 has a total capacity of 10.75 GiB of which 4.98 GiB is free. Including non-PyTorch memory, this process has 5.77 GiB memory in use. Of the allocated memory 5.59 GiB is allocated by PyTorch, and 3.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-21 14:33:17,206 - ERROR - G&C failed for student_dim=150, seed=551: CUDA out of memory. Tried to allocate 22.35 GiB. GPU 3 has a total capacity of 10.75 GiB of which 4.98 GiB is free. Including non-PyTorch memory, this process has 5.77 GiB memory in use. Of the allocated memory 5.59 GiB is allocated by PyTorch, and 3.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-21 14:33:17,215 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 100, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size, False)
                                        ^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 157, in train_gnc
    train_loss, gen_loss = get_losses(students, w_sequences, alpha_teacher)
                           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 94, in get_losses
    A_pows = torch.empty(batch_size, state_dim, M, device=device, dtype=dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.35 GiB. GPU 0 has a total capacity of 10.75 GiB of which 4.98 GiB is free. Including non-PyTorch memory, this process has 5.77 GiB memory in use. Of the allocated memory 5.59 GiB is allocated by PyTorch, and 3.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-10-21 14:33:17,215 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 100, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size, False)
                                        ^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 157, in train_gnc
    train_loss, gen_loss = get_losses(students, w_sequences, alpha_teacher)
                           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 94, in get_losses
    A_pows = torch.empty(batch_size, state_dim, M, device=device, dtype=dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.35 GiB. GPU 1 has a total capacity of 10.75 GiB of which 4.98 GiB is free. Including non-PyTorch memory, this process has 5.77 GiB memory in use. Of the allocated memory 5.59 GiB is allocated by PyTorch, and 3.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-10-21 14:33:17,215 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 100, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size, False)
                                        ^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 157, in train_gnc
    train_loss, gen_loss = get_losses(students, w_sequences, alpha_teacher)
                           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 94, in get_losses
    A_pows = torch.empty(batch_size, state_dim, M, device=device, dtype=dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.35 GiB. GPU 3 has a total capacity of 10.75 GiB of which 4.98 GiB is free. Including non-PyTorch memory, this process has 5.77 GiB memory in use. Of the allocated memory 5.59 GiB is allocated by PyTorch, and 3.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-10-21 14:33:17,216 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 100, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size, False)
                                        ^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 157, in train_gnc
    train_loss, gen_loss = get_losses(students, w_sequences, alpha_teacher)
                           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 94, in get_losses
    A_pows = torch.empty(batch_size, state_dim, M, device=device, dtype=dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.35 GiB. GPU 2 has a total capacity of 10.75 GiB of which 4.98 GiB is free. Including non-PyTorch memory, this process has 5.77 GiB memory in use. Of the allocated memory 5.59 GiB is allocated by PyTorch, and 3.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-10-21 14:33:17,228 - INFO - Process 0: Completed experiment - student_dim=150, seed=774
2025-10-21 14:33:17,237 - INFO - Process 3: Completed experiment - student_dim=150, seed=551
2025-10-21 14:33:17,244 - INFO - Process 2: Completed experiment - student_dim=150, seed=744
2025-10-21 14:33:17,253 - INFO - Process 1: Completed experiment - student_dim=150, seed=242
2025-10-21 14:33:17,432 - ERROR - G&C failed for student_dim=200, seed=774: CUDA out of memory. Tried to allocate 29.80 GiB. GPU 0 has a total capacity of 10.75 GiB of which 3.12 GiB is free. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-21 14:33:17,432 - ERROR - G&C failed for student_dim=200, seed=744: CUDA out of memory. Tried to allocate 29.80 GiB. GPU 2 has a total capacity of 10.75 GiB of which 3.12 GiB is free. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-21 14:33:17,433 - ERROR - G&C failed for student_dim=200, seed=242: CUDA out of memory. Tried to allocate 29.80 GiB. GPU 1 has a total capacity of 10.75 GiB of which 3.12 GiB is free. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-21 14:33:17,433 - ERROR - G&C failed for student_dim=200, seed=551: CUDA out of memory. Tried to allocate 29.80 GiB. GPU 3 has a total capacity of 10.75 GiB of which 3.12 GiB is free. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-21 14:33:17,433 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 100, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size, False)
                                        ^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 157, in train_gnc
    train_loss, gen_loss = get_losses(students, w_sequences, alpha_teacher)
                           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 94, in get_losses
    A_pows = torch.empty(batch_size, state_dim, M, device=device, dtype=dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 29.80 GiB. GPU 2 has a total capacity of 10.75 GiB of which 3.12 GiB is free. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-10-21 14:33:17,434 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 100, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size, False)
                                        ^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 157, in train_gnc
    train_loss, gen_loss = get_losses(students, w_sequences, alpha_teacher)
                           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 94, in get_losses
    A_pows = torch.empty(batch_size, state_dim, M, device=device, dtype=dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 29.80 GiB. GPU 0 has a total capacity of 10.75 GiB of which 3.12 GiB is free. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-10-21 14:33:17,434 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 100, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size, False)
                                        ^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 157, in train_gnc
    train_loss, gen_loss = get_losses(students, w_sequences, alpha_teacher)
                           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 94, in get_losses
    A_pows = torch.empty(batch_size, state_dim, M, device=device, dtype=dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 29.80 GiB. GPU 1 has a total capacity of 10.75 GiB of which 3.12 GiB is free. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-10-21 14:33:17,435 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 100, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size, False)
                                        ^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 157, in train_gnc
    train_loss, gen_loss = get_losses(students, w_sequences, alpha_teacher)
                           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 94, in get_losses
    A_pows = torch.empty(batch_size, state_dim, M, device=device, dtype=dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 29.80 GiB. GPU 3 has a total capacity of 10.75 GiB of which 3.12 GiB is free. Including non-PyTorch memory, this process has 7.63 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-10-21 14:33:17,446 - INFO - Process 0: Completed experiment - student_dim=200, seed=774
2025-10-21 14:33:17,453 - INFO - Process 3: Completed experiment - student_dim=200, seed=551
2025-10-21 14:33:17,458 - INFO - Process 1: Completed experiment - student_dim=200, seed=242
2025-10-21 14:33:17,472 - INFO - Process 2: Completed experiment - student_dim=200, seed=744
2025-10-21 14:33:17,472 - INFO - Process 3 completed on GPU 3
2025-10-21 14:33:17,472 - INFO - Process 0 completed on GPU 0
2025-10-21 14:33:17,473 - INFO - Process 1 completed on GPU 1
2025-10-21 14:33:17,473 - INFO - Process 2 completed on GPU 2
2025-10-21 14:33:21,491 - INFO - Process 2 completed
2025-10-21 14:33:22,492 - INFO - Process 0 completed
2025-10-21 14:33:23,494 - INFO - Process 1 completed
2025-10-21 14:33:24,495 - INFO - Process 3 completed
2025-10-21 14:33:25,520 - INFO - Checkpoint saved: test_results/checkpoints/checkpoint_20251021_143325.pkl (0.0% complete)
2025-10-21 14:33:25,520 - INFO - Final checkpoint saved
2025-10-21 14:33:25,544 - INFO - Results saved to results__gnc_seq_len=5_num_seq=2_seeds=774-242-744-551_time=20251021_143311.csv
2025-10-21 14:33:36,748 - INFO - Figures saved to plot__gnc_seq_len=5_num_seq=2_seeds=774-242-744-551_time=20251021_143311
2025-10-21 14:33:36,749 - INFO - Finished experiments, results saved to results__gnc_seq_len=5_num_seq=2_seeds=774-242-744-551_time=20251021_143311.csv, figures saved to plot__gnc_seq_len=5_num_seq=2_seeds=774-242-744-551_time=20251021_143311
