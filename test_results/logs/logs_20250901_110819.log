2025-09-01 11:08:19,632 - INFO - Args: Namespace(num_seeds=5, seeds=[39324, 33186, 85955, 18678, 37362], sequence_length=5, num_sequences=1, student_dims=[150, 175, 200, 225, 250, 275], eps_train=1e-05, w_that_minimizes_loss=False, gnc=True, gnc_num_samples=100000000, gnc_batch_size=1000000, gd=True, gd_lr=0.001, gd_epochs=10000, gd_init_scale=0.01, gd_optimizer='adam', gd_scheduler=None, gd_scheduler_params='{}', exp_gamma=None, step_size=None, step_gamma=None, cosine_eta_min=None, gd_init_type='regular', config=None, results_dir=PosixPath('test_results/results'), figures_dir=PosixPath('test_results/figures'), checkpoint_dir=PosixPath('test_results/checkpoints'), checkpoint_interval=3600, resume_from_checkpoint=False, log_dir=PosixPath('test_results/logs'), max_gpus=4, log_file=PosixPath('test_results/logs/logs_20250901_110819.log'))
2025-09-01 11:08:19,634 - INFO - Using custom seeds: [39324, 33186, 85955, 18678, 37362]
2025-09-01 11:08:19,838 - INFO - Using GPUs: [0, 1, 2, 3]
2025-09-01 11:08:19,838 - INFO - Starting 4 processes on 4 GPUs
2025-09-01 11:08:27,508 - INFO - Process 0 started on GPU 0, processing seeds [39324, 33186]
2025-09-01 11:08:28,062 - INFO - Process 2 started on GPU 2, processing seeds [18678]
2025-09-01 11:08:28,076 - INFO - Process 1 started on GPU 1, processing seeds [85955]
2025-09-01 11:08:28,086 - INFO - Process 3 started on GPU 3, processing seeds [37362]
2025-09-01 11:08:28,247 - INFO - for seed 39324, alpha_teacher=tensor([0.5566], device='cuda:0'), generated 1 sequences
2025-09-01 11:08:28,247 - INFO - Process 0: Starting experiment - student_dim=150, seed=39324
2025-09-01 11:08:28,264 - INFO - for seed 18678, alpha_teacher=tensor([0.4602], device='cuda:2'), generated 1 sequences
2025-09-01 11:08:28,264 - INFO - Process 2: Starting experiment - student_dim=150, seed=18678
2025-09-01 11:08:28,280 - INFO - for seed 85955, alpha_teacher=tensor([0.6314], device='cuda:1'), generated 1 sequences
2025-09-01 11:08:28,281 - INFO - Process 1: Starting experiment - student_dim=150, seed=85955
2025-09-01 11:08:28,304 - INFO - for seed 37362, alpha_teacher=tensor([0.5143], device='cuda:3'), generated 1 sequences
2025-09-01 11:08:28,304 - INFO - Process 3: Starting experiment - student_dim=150, seed=37362
2025-09-01 11:13:14,530 - INFO - delta_l_infinity: tensor([0.1972], device='cuda:0') for w=tensor([-1.1435,  1.3301, -2.0590,  0.6975], device='cuda:0')
2025-09-01 11:13:14,532 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:13:14,532 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:13:19,450 - INFO - initial model: max A_j index: 56
2025-09-01 11:13:37,849 - INFO - delta_l_infinity: tensor([-20.3177], device='cuda:2') for w=tensor([ 0.2387, -1.1835,  1.4965,  0.6423], device='cuda:2')
2025-09-01 11:13:37,850 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:13:37,850 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:13:39,883 - INFO - initial model: max A_j index: 138
2025-09-01 11:13:48,248 - INFO - final model: max A_j index: 56, max A_j value: 0.03240163251757622, alpha_teacher: tensor([0.5566], device='cuda:0')
2025-09-01 11:13:48,311 - INFO - largest 10 A_j values: tensor([0.0324, 0.0246, 0.0243, 0.0232, 0.0219, 0.0215, 0.0198, 0.0191, 0.0163,
        0.0160], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 11:13:48,312 - INFO - number of values below 0.01: 119
2025-09-01 11:13:48,312 - INFO - number of values between 0.01 and 0.1: 31
2025-09-01 11:13:48,312 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:13:48,312 - INFO - number of values larger than 0.3: 0
2025-09-01 11:13:48,312 - INFO - train loss is 0.0
2025-09-01 11:13:48,312 - INFO - impulse response loss is 0.1353078931570053
2025-09-01 11:13:48,320 - INFO - Process 0: Completed experiment - student_dim=150, seed=39324
2025-09-01 11:13:54,161 - INFO - delta_l_infinity: tensor([0.9729], device='cuda:3') for w=tensor([-2.0040, -1.4912, -0.4689,  0.7015], device='cuda:3')
2025-09-01 11:13:54,162 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:13:54,162 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:13:55,991 - INFO - initial model: max A_j index: 114
2025-09-01 11:14:08,879 - INFO - final model: max A_j index: 138, max A_j value: 0.029306137934327126, alpha_teacher: tensor([0.4602], device='cuda:2')
2025-09-01 11:14:08,941 - INFO - largest 10 A_j values: tensor([0.0293, 0.0280, 0.0260, 0.0246, 0.0232, 0.0213, 0.0197, 0.0196, 0.0195,
        0.0193], device='cuda:2', grad_fn=<TopkBackward0>)
2025-09-01 11:14:08,941 - INFO - number of values below 0.01: 123
2025-09-01 11:14:08,942 - INFO - number of values between 0.01 and 0.1: 27
2025-09-01 11:14:08,942 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:14:08,942 - INFO - number of values larger than 0.3: 0
2025-09-01 11:14:08,942 - INFO - train loss is 0.0
2025-09-01 11:14:08,942 - INFO - impulse response loss is 0.10440728813409805
2025-09-01 11:14:08,951 - INFO - Process 2: Completed experiment - student_dim=150, seed=18678
2025-09-01 11:14:24,863 - INFO - final model: max A_j index: 114, max A_j value: 0.032572485506534576, alpha_teacher: tensor([0.5143], device='cuda:3')
2025-09-01 11:14:24,925 - INFO - largest 10 A_j values: tensor([0.0326, 0.0298, 0.0292, 0.0250, 0.0241, 0.0235, 0.0226, 0.0212, 0.0211,
        0.0209], device='cuda:3', grad_fn=<TopkBackward0>)
2025-09-01 11:14:24,925 - INFO - number of values below 0.01: 106
2025-09-01 11:14:24,925 - INFO - number of values between 0.01 and 0.1: 44
2025-09-01 11:14:24,925 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:14:24,926 - INFO - number of values larger than 0.3: 0
2025-09-01 11:14:24,926 - INFO - train loss is 4.7493259325293025e-12
2025-09-01 11:14:24,926 - INFO - impulse response loss is 0.11962781846523285
2025-09-01 11:14:24,933 - INFO - Process 3: Completed experiment - student_dim=150, seed=37362
2025-09-01 11:15:27,850 - INFO - delta_l_infinity: tensor([0.9771], device='cuda:1') for w=tensor([-0.6442,  0.2456, -0.4569, -0.9790], device='cuda:1')
2025-09-01 11:15:27,853 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:15:27,853 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:15:31,223 - INFO - initial model: max A_j index: 132
2025-09-01 11:15:58,844 - INFO - final model: max A_j index: 132, max A_j value: 0.030964305624365807, alpha_teacher: tensor([0.6314], device='cuda:1')
2025-09-01 11:15:58,901 - INFO - largest 10 A_j values: tensor([0.0310, 0.0297, 0.0283, 0.0265, 0.0262, 0.0262, 0.0260, 0.0259, 0.0241,
        0.0236], device='cuda:1', grad_fn=<TopkBackward0>)
2025-09-01 11:15:58,902 - INFO - number of values below 0.01: 102
2025-09-01 11:15:58,902 - INFO - number of values between 0.01 and 0.1: 48
2025-09-01 11:15:58,902 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:15:58,902 - INFO - number of values larger than 0.3: 0
2025-09-01 11:15:58,902 - INFO - train loss is 0.0
2025-09-01 11:15:58,902 - INFO - impulse response loss is 0.3067018985748291
2025-09-01 11:15:58,908 - INFO - Process 1: Completed experiment - student_dim=150, seed=85955
2025-09-01 11:19:25,954 - INFO - delta_l_infinity: tensor([0.1972], device='cuda:0') for w=tensor([-1.1435,  1.3301, -2.0590,  0.6975], device='cuda:0')
2025-09-01 11:19:25,954 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:19:25,955 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:19:25,955 - INFO - initial model: max A_j index: 56
2025-09-01 11:19:54,679 - INFO - final model: max A_j index: 56, max A_j value: 0.031918879598379135, alpha_teacher: tensor([0.5566], device='cuda:0')
2025-09-01 11:19:54,681 - INFO - largest 10 A_j values: tensor([0.0319, 0.0293, 0.0242, 0.0238, 0.0228, 0.0215, 0.0195, 0.0186, 0.0180,
        0.0177], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 11:19:54,681 - INFO - number of values below 0.01: 139
2025-09-01 11:19:54,682 - INFO - number of values between 0.01 and 0.1: 36
2025-09-01 11:19:54,682 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:19:54,682 - INFO - number of values larger than 0.3: 0
2025-09-01 11:19:54,683 - INFO - train loss is 1.3656631381309126e-11
2025-09-01 11:19:54,683 - INFO - impulse response loss is 0.13320499658584595
2025-09-01 11:19:54,695 - INFO - Process 0: Completed experiment - student_dim=175, seed=39324
2025-09-01 11:20:14,196 - INFO - delta_l_infinity: tensor([-20.3177], device='cuda:2') for w=tensor([ 0.2387, -1.1835,  1.4965,  0.6423], device='cuda:2')
2025-09-01 11:20:14,197 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:20:14,197 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:20:14,198 - INFO - initial model: max A_j index: 144
2025-09-01 11:20:42,951 - INFO - final model: max A_j index: 144, max A_j value: 0.02894746884703636, alpha_teacher: tensor([0.4602], device='cuda:2')
2025-09-01 11:20:42,953 - INFO - largest 10 A_j values: tensor([0.0289, 0.0271, 0.0271, 0.0251, 0.0237, 0.0223, 0.0204, 0.0188, 0.0187,
        0.0186], device='cuda:2', grad_fn=<TopkBackward0>)
2025-09-01 11:20:42,954 - INFO - number of values below 0.01: 142
2025-09-01 11:20:42,954 - INFO - number of values between 0.01 and 0.1: 33
2025-09-01 11:20:42,954 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:20:42,954 - INFO - number of values larger than 0.3: 0
2025-09-01 11:20:42,955 - INFO - train loss is 0.0
2025-09-01 11:20:42,955 - INFO - impulse response loss is 0.09696733206510544
2025-09-01 11:20:42,967 - INFO - Process 2: Completed experiment - student_dim=175, seed=18678
2025-09-01 11:21:05,060 - INFO - delta_l_infinity: tensor([0.9729], device='cuda:3') for w=tensor([-2.0040, -1.4912, -0.4689,  0.7015], device='cuda:3')
2025-09-01 11:21:05,061 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:21:05,061 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:21:05,061 - INFO - initial model: max A_j index: 114
2025-09-01 11:21:33,816 - INFO - final model: max A_j index: 114, max A_j value: 0.031201902776956558, alpha_teacher: tensor([0.5143], device='cuda:3')
2025-09-01 11:21:33,818 - INFO - largest 10 A_j values: tensor([0.0312, 0.0285, 0.0278, 0.0249, 0.0236, 0.0228, 0.0227, 0.0221, 0.0212,
        0.0200], device='cuda:3', grad_fn=<TopkBackward0>)
2025-09-01 11:21:33,818 - INFO - number of values below 0.01: 131
2025-09-01 11:21:33,819 - INFO - number of values between 0.01 and 0.1: 44
2025-09-01 11:21:33,819 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:21:33,819 - INFO - number of values larger than 0.3: 0
2025-09-01 11:21:33,819 - INFO - train loss is 1.5879504644544795e-05
2025-09-01 11:21:33,819 - INFO - impulse response loss is 0.12065780162811279
2025-09-01 11:21:33,832 - INFO - Process 3: Completed experiment - student_dim=175, seed=37362
2025-09-01 11:24:35,327 - INFO - delta_l_infinity: tensor([0.9771], device='cuda:1') for w=tensor([-0.6442,  0.2456, -0.4569, -0.9790], device='cuda:1')
2025-09-01 11:24:35,328 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:24:35,329 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:24:35,330 - INFO - initial model: max A_j index: 132
2025-09-01 11:25:01,226 - INFO - final model: max A_j index: 132, max A_j value: 0.02917833998799324, alpha_teacher: tensor([0.6314], device='cuda:1')
2025-09-01 11:25:01,228 - INFO - largest 10 A_j values: tensor([0.0292, 0.0265, 0.0250, 0.0247, 0.0247, 0.0244, 0.0244, 0.0243, 0.0241,
        0.0231], device='cuda:1', grad_fn=<TopkBackward0>)
2025-09-01 11:25:01,229 - INFO - number of values below 0.01: 124
2025-09-01 11:25:01,229 - INFO - number of values between 0.01 and 0.1: 51
2025-09-01 11:25:01,230 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:25:01,230 - INFO - number of values larger than 0.3: 0
2025-09-01 11:25:01,230 - INFO - train loss is 0.0
2025-09-01 11:25:01,230 - INFO - impulse response loss is 0.30612045526504517
2025-09-01 11:25:01,359 - INFO - Process 1: Completed experiment - student_dim=175, seed=85955
2025-09-01 11:26:21,367 - INFO - delta_l_infinity: tensor([0.1972], device='cuda:0') for w=tensor([-1.1435,  1.3301, -2.0590,  0.6975], device='cuda:0')
2025-09-01 11:26:21,368 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:26:21,368 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:26:21,368 - INFO - initial model: max A_j index: 56
2025-09-01 11:26:49,729 - INFO - final model: max A_j index: 56, max A_j value: 0.032424796372652054, alpha_teacher: tensor([0.5566], device='cuda:0')
2025-09-01 11:26:49,731 - INFO - largest 10 A_j values: tensor([0.0324, 0.0263, 0.0247, 0.0243, 0.0233, 0.0220, 0.0191, 0.0185, 0.0183,
        0.0182], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 11:26:49,731 - INFO - number of values below 0.01: 157
2025-09-01 11:26:49,732 - INFO - number of values between 0.01 and 0.1: 43
2025-09-01 11:26:49,732 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:26:49,732 - INFO - number of values larger than 0.3: 0
2025-09-01 11:26:49,732 - INFO - train loss is 1.0455424082067566e-08
2025-09-01 11:26:49,732 - INFO - impulse response loss is 0.13155585527420044
2025-09-01 11:26:49,746 - INFO - Process 0: Completed experiment - student_dim=200, seed=39324
2025-09-01 11:27:40,859 - INFO - delta_l_infinity: tensor([-20.3177], device='cuda:2') for w=tensor([ 0.2387, -1.1835,  1.4965,  0.6423], device='cuda:2')
2025-09-01 11:27:40,859 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:27:40,860 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:27:40,860 - INFO - initial model: max A_j index: 190
2025-09-01 11:28:09,656 - INFO - final model: max A_j index: 190, max A_j value: 0.031264983117580414, alpha_teacher: tensor([0.4602], device='cuda:2')
2025-09-01 11:28:09,659 - INFO - largest 10 A_j values: tensor([0.0313, 0.0285, 0.0267, 0.0267, 0.0247, 0.0233, 0.0219, 0.0218, 0.0209,
        0.0203], device='cuda:2', grad_fn=<TopkBackward0>)
2025-09-01 11:28:09,659 - INFO - number of values below 0.01: 161
2025-09-01 11:28:09,660 - INFO - number of values between 0.01 and 0.1: 39
2025-09-01 11:28:09,660 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:28:09,660 - INFO - number of values larger than 0.3: 0
2025-09-01 11:28:09,660 - INFO - train loss is 0.0
2025-09-01 11:28:09,660 - INFO - impulse response loss is 0.08500155806541443
2025-09-01 11:28:09,673 - INFO - Process 2: Completed experiment - student_dim=200, seed=18678
2025-09-01 11:29:06,866 - INFO - delta_l_infinity: tensor([0.9729], device='cuda:3') for w=tensor([-2.0040, -1.4912, -0.4689,  0.7015], device='cuda:3')
2025-09-01 11:29:06,866 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:29:06,867 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:29:06,867 - INFO - initial model: max A_j index: 114
2025-09-01 11:29:35,676 - INFO - final model: max A_j index: 114, max A_j value: 0.03062436357140541, alpha_teacher: tensor([0.5143], device='cuda:3')
2025-09-01 11:29:35,678 - INFO - largest 10 A_j values: tensor([0.0306, 0.0290, 0.0279, 0.0272, 0.0243, 0.0236, 0.0231, 0.0222, 0.0221,
        0.0215], device='cuda:3', grad_fn=<TopkBackward0>)
2025-09-01 11:29:35,678 - INFO - number of values below 0.01: 150
2025-09-01 11:29:35,679 - INFO - number of values between 0.01 and 0.1: 50
2025-09-01 11:29:35,679 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:29:35,679 - INFO - number of values larger than 0.3: 0
2025-09-01 11:29:35,680 - INFO - train loss is 2.8102520310824275e-14
2025-09-01 11:29:35,680 - INFO - impulse response loss is 0.11623042821884155
2025-09-01 11:29:35,693 - INFO - Process 3: Completed experiment - student_dim=200, seed=37362
2025-09-01 11:34:04,596 - INFO - delta_l_infinity: tensor([0.1972], device='cuda:0') for w=tensor([-1.1435,  1.3301, -2.0590,  0.6975], device='cuda:0')
2025-09-01 11:34:04,597 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:34:04,597 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:34:04,598 - INFO - initial model: max A_j index: 56
2025-09-01 11:34:23,113 - INFO - final model: max A_j index: 56, max A_j value: 0.03231291100382805, alpha_teacher: tensor([0.5566], device='cuda:0')
2025-09-01 11:34:23,114 - INFO - largest 10 A_j values: tensor([0.0323, 0.0310, 0.0297, 0.0245, 0.0242, 0.0231, 0.0218, 0.0197, 0.0190,
        0.0184], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 11:34:23,115 - INFO - number of values below 0.01: 181
2025-09-01 11:34:23,115 - INFO - number of values between 0.01 and 0.1: 44
2025-09-01 11:34:23,115 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:34:23,115 - INFO - number of values larger than 0.3: 0
2025-09-01 11:34:23,115 - INFO - train loss is 0.0
2025-09-01 11:34:23,115 - INFO - impulse response loss is 0.129351407289505
2025-09-01 11:34:23,121 - INFO - Process 0: Completed experiment - student_dim=225, seed=39324
2025-09-01 11:34:27,874 - ERROR - G&C failed for student_dim=250, seed=39324: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 11:34:27,881 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 11:34:27,881 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:34:27,881 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:34:27,882 - INFO - initial model: max A_j index: 56
2025-09-01 11:34:49,982 - INFO - final model: max A_j index: 56, max A_j value: 0.03220575675368309, alpha_teacher: tensor([0.5566], device='cuda:0')
2025-09-01 11:34:49,984 - INFO - largest 10 A_j values: tensor([0.0322, 0.0296, 0.0244, 0.0241, 0.0230, 0.0217, 0.0193, 0.0189, 0.0187,
        0.0183], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 11:34:49,984 - INFO - number of values below 0.01: 204
2025-09-01 11:34:49,984 - INFO - number of values between 0.01 and 0.1: 46
2025-09-01 11:34:49,985 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:34:49,985 - INFO - number of values larger than 0.3: 0
2025-09-01 11:34:49,985 - INFO - train loss is 0.0
2025-09-01 11:34:49,985 - INFO - impulse response loss is 0.12879744172096252
2025-09-01 11:34:49,997 - INFO - Process 0: Completed experiment - student_dim=250, seed=39324
2025-09-01 11:34:55,261 - ERROR - G&C failed for student_dim=275, seed=39324: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 0 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 11:34:55,264 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 0 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 11:34:55,265 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:34:55,265 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:34:55,266 - INFO - initial model: max A_j index: 56
2025-09-01 11:34:56,609 - INFO - delta_l_infinity: tensor([0.9771], device='cuda:1') for w=tensor([-0.6442,  0.2456, -0.4569, -0.9790], device='cuda:1')
2025-09-01 11:34:56,610 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:34:56,610 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:34:56,610 - INFO - initial model: max A_j index: 160
2025-09-01 11:35:13,337 - INFO - final model: max A_j index: 160, max A_j value: 0.03605572506785393, alpha_teacher: tensor([0.6314], device='cuda:1')
2025-09-01 11:35:13,338 - INFO - largest 10 A_j values: tensor([0.0361, 0.0289, 0.0262, 0.0247, 0.0244, 0.0243, 0.0241, 0.0241, 0.0239,
        0.0238], device='cuda:1', grad_fn=<TopkBackward0>)
2025-09-01 11:35:13,338 - INFO - number of values below 0.01: 142
2025-09-01 11:35:13,338 - INFO - number of values between 0.01 and 0.1: 58
2025-09-01 11:35:13,339 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:35:13,339 - INFO - number of values larger than 0.3: 0
2025-09-01 11:35:13,339 - INFO - train loss is 0.0
2025-09-01 11:35:13,339 - INFO - impulse response loss is 0.3044127821922302
2025-09-01 11:35:13,509 - INFO - Process 1: Completed experiment - student_dim=200, seed=85955
2025-09-01 11:35:24,180 - INFO - final model: max A_j index: 56, max A_j value: 0.03180833160877228, alpha_teacher: tensor([0.5566], device='cuda:0')
2025-09-01 11:35:24,182 - INFO - largest 10 A_j values: tensor([0.0318, 0.0305, 0.0292, 0.0240, 0.0237, 0.0226, 0.0213, 0.0212, 0.0206,
        0.0192], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 11:35:24,182 - INFO - number of values below 0.01: 220
2025-09-01 11:35:24,182 - INFO - number of values between 0.01 and 0.1: 55
2025-09-01 11:35:24,183 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:35:24,183 - INFO - number of values larger than 0.3: 0
2025-09-01 11:35:24,183 - INFO - train loss is 0.0
2025-09-01 11:35:24,183 - INFO - impulse response loss is 0.1256759762763977
2025-09-01 11:35:24,196 - INFO - Process 0: Completed experiment - student_dim=275, seed=39324
2025-09-01 11:35:24,204 - INFO - for seed 33186, alpha_teacher=tensor([0.4665], device='cuda:0'), generated 1 sequences
2025-09-01 11:36:00,626 - INFO - delta_l_infinity: tensor([-20.3177], device='cuda:2') for w=tensor([ 0.2387, -1.1835,  1.4965,  0.6423], device='cuda:2')
2025-09-01 11:36:00,626 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:36:00,626 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:36:00,627 - INFO - initial model: max A_j index: 144
2025-09-01 11:36:11,235 - INFO - final model: max A_j index: 144, max A_j value: 0.02818956784904003, alpha_teacher: tensor([0.4602], device='cuda:2')
2025-09-01 11:36:11,236 - INFO - largest 10 A_j values: tensor([0.0282, 0.0264, 0.0263, 0.0252, 0.0243, 0.0231, 0.0230, 0.0224, 0.0215,
        0.0214], device='cuda:2', grad_fn=<TopkBackward0>)
2025-09-01 11:36:11,236 - INFO - number of values below 0.01: 188
2025-09-01 11:36:11,236 - INFO - number of values between 0.01 and 0.1: 37
2025-09-01 11:36:11,236 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:36:11,236 - INFO - number of values larger than 0.3: 0
2025-09-01 11:36:11,236 - INFO - train loss is 0.0
2025-09-01 11:36:11,236 - INFO - impulse response loss is 0.0855371505022049
2025-09-01 11:36:11,242 - INFO - Process 2: Completed experiment - student_dim=225, seed=18678
2025-09-01 11:36:16,212 - ERROR - G&C failed for student_dim=250, seed=18678: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 2 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 11:36:16,220 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 2 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 11:36:16,220 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:36:16,220 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:36:16,221 - INFO - initial model: max A_j index: 144
2025-09-01 11:36:36,174 - INFO - final model: max A_j index: 144, max A_j value: 0.02803088165819645, alpha_teacher: tensor([0.4602], device='cuda:2')
2025-09-01 11:36:36,176 - INFO - largest 10 A_j values: tensor([0.0280, 0.0266, 0.0262, 0.0262, 0.0242, 0.0230, 0.0228, 0.0223, 0.0213,
        0.0213], device='cuda:2', grad_fn=<TopkBackward0>)
2025-09-01 11:36:36,176 - INFO - number of values below 0.01: 204
2025-09-01 11:36:36,177 - INFO - number of values between 0.01 and 0.1: 46
2025-09-01 11:36:36,177 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:36:36,177 - INFO - number of values larger than 0.3: 0
2025-09-01 11:36:36,177 - INFO - train loss is 0.0
2025-09-01 11:36:36,178 - INFO - impulse response loss is 0.07562293857336044
2025-09-01 11:36:36,191 - INFO - Process 2: Completed experiment - student_dim=250, seed=18678
2025-09-01 11:36:41,669 - ERROR - G&C failed for student_dim=275, seed=18678: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 2 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 11:36:41,672 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 2 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 11:36:41,672 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:36:41,673 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:36:41,674 - INFO - initial model: max A_j index: 144
2025-09-01 11:37:10,260 - INFO - final model: max A_j index: 144, max A_j value: 0.028322165831923485, alpha_teacher: tensor([0.4602], device='cuda:2')
2025-09-01 11:37:10,263 - INFO - largest 10 A_j values: tensor([0.0283, 0.0268, 0.0265, 0.0264, 0.0253, 0.0244, 0.0232, 0.0230, 0.0225,
        0.0221], device='cuda:2', grad_fn=<TopkBackward0>)
2025-09-01 11:37:10,263 - INFO - number of values below 0.01: 225
2025-09-01 11:37:10,263 - INFO - number of values between 0.01 and 0.1: 50
2025-09-01 11:37:10,264 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:37:10,264 - INFO - number of values larger than 0.3: 0
2025-09-01 11:37:10,264 - INFO - train loss is 9.540817913489263e-09
2025-09-01 11:37:10,264 - INFO - impulse response loss is 0.07265058159828186
2025-09-01 11:37:10,278 - INFO - Process 2: Completed experiment - student_dim=275, seed=18678
2025-09-01 11:37:10,280 - INFO - Process 2 completed on GPU 2
2025-09-01 11:37:16,980 - INFO - Process 2 completed
2025-09-01 11:38:07,123 - INFO - delta_l_infinity: tensor([0.9729], device='cuda:3') for w=tensor([-2.0040, -1.4912, -0.4689,  0.7015], device='cuda:3')
2025-09-01 11:38:07,123 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:38:07,123 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:38:07,124 - INFO - initial model: max A_j index: 114
2025-09-01 11:38:36,070 - INFO - final model: max A_j index: 114, max A_j value: 0.030231762677431107, alpha_teacher: tensor([0.5143], device='cuda:3')
2025-09-01 11:38:36,072 - INFO - largest 10 A_j values: tensor([0.0302, 0.0286, 0.0275, 0.0268, 0.0258, 0.0239, 0.0232, 0.0227, 0.0218,
        0.0218], device='cuda:3', grad_fn=<TopkBackward0>)
2025-09-01 11:38:36,073 - INFO - number of values below 0.01: 169
2025-09-01 11:38:36,073 - INFO - number of values between 0.01 and 0.1: 56
2025-09-01 11:38:36,073 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:38:36,074 - INFO - number of values larger than 0.3: 0
2025-09-01 11:38:36,074 - INFO - train loss is 1.6903284327796086e-12
2025-09-01 11:38:36,074 - INFO - impulse response loss is 0.11515001207590103
2025-09-01 11:38:36,085 - INFO - Process 3: Completed experiment - student_dim=225, seed=37362
2025-09-01 11:38:40,896 - ERROR - G&C failed for student_dim=250, seed=37362: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 3 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 11:38:40,904 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 3 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 11:38:40,904 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:38:40,904 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:38:40,905 - INFO - initial model: max A_j index: 114
2025-09-01 11:39:09,822 - INFO - final model: max A_j index: 114, max A_j value: 0.030273843556642532, alpha_teacher: tensor([0.5143], device='cuda:3')
2025-09-01 11:39:09,824 - INFO - largest 10 A_j values: tensor([0.0303, 0.0286, 0.0275, 0.0269, 0.0259, 0.0240, 0.0232, 0.0227, 0.0219,
        0.0218], device='cuda:3', grad_fn=<TopkBackward0>)
2025-09-01 11:39:09,825 - INFO - number of values below 0.01: 193
2025-09-01 11:39:09,825 - INFO - number of values between 0.01 and 0.1: 57
2025-09-01 11:39:09,825 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:39:09,826 - INFO - number of values larger than 0.3: 0
2025-09-01 11:39:09,826 - INFO - train loss is 8.127387651768458e-13
2025-09-01 11:39:09,826 - INFO - impulse response loss is 0.11347289383411407
2025-09-01 11:39:09,839 - INFO - Process 3: Completed experiment - student_dim=250, seed=37362
2025-09-01 11:39:15,177 - ERROR - G&C failed for student_dim=275, seed=37362: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 3 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 11:39:15,179 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 3 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 11:39:15,180 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:39:15,180 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:39:15,181 - INFO - initial model: max A_j index: 114
2025-09-01 11:39:43,889 - INFO - final model: max A_j index: 114, max A_j value: 0.029866334050893784, alpha_teacher: tensor([0.5143], device='cuda:3')
2025-09-01 11:39:43,891 - INFO - largest 10 A_j values: tensor([0.0299, 0.0282, 0.0271, 0.0265, 0.0255, 0.0248, 0.0236, 0.0228, 0.0224,
        0.0223], device='cuda:3', grad_fn=<TopkBackward0>)
2025-09-01 11:39:43,891 - INFO - number of values below 0.01: 211
2025-09-01 11:39:43,892 - INFO - number of values between 0.01 and 0.1: 64
2025-09-01 11:39:43,892 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:39:43,892 - INFO - number of values larger than 0.3: 0
2025-09-01 11:39:43,892 - INFO - train loss is 3.5649367191581405e-07
2025-09-01 11:39:43,892 - INFO - impulse response loss is 0.11256773769855499
2025-09-01 11:39:43,906 - INFO - Process 3: Completed experiment - student_dim=275, seed=37362
2025-09-01 11:39:43,907 - INFO - Process 3 completed on GPU 3
2025-09-01 11:39:49,989 - INFO - Process 3 completed
2025-09-01 11:40:11,895 - INFO - delta_l_infinity: tensor([1.2171], device='cuda:0') for w=tensor([ 1.7525, -0.1222, -0.0972, -0.9637], device='cuda:0')
2025-09-01 11:40:11,895 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:40:11,895 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:40:11,896 - INFO - initial model: max A_j index: 134
2025-09-01 11:40:41,076 - INFO - final model: max A_j index: 134, max A_j value: 0.03469737619161606, alpha_teacher: tensor([0.4665], device='cuda:0')
2025-09-01 11:40:41,078 - INFO - largest 10 A_j values: tensor([0.0347, 0.0295, 0.0269, 0.0264, 0.0258, 0.0232, 0.0216, 0.0208, 0.0198,
        0.0192], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 11:40:41,079 - INFO - number of values below 0.01: 115
2025-09-01 11:40:41,079 - INFO - number of values between 0.01 and 0.1: 35
2025-09-01 11:40:41,079 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:40:41,080 - INFO - number of values larger than 0.3: 0
2025-09-01 11:40:41,080 - INFO - train loss is 4.1907064769475255e-06
2025-09-01 11:40:41,080 - INFO - impulse response loss is 0.054165832698345184
2025-09-01 11:40:41,091 - INFO - Process 0: Completed experiment - student_dim=150, seed=33186
2025-09-01 11:46:19,476 - INFO - delta_l_infinity: tensor([1.2171], device='cuda:0') for w=tensor([ 1.7525, -0.1222, -0.0972, -0.9637], device='cuda:0')
2025-09-01 11:46:19,477 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:46:19,477 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:46:19,478 - INFO - initial model: max A_j index: 150
2025-09-01 11:46:30,813 - INFO - final model: max A_j index: 150, max A_j value: 0.03427812457084656, alpha_teacher: tensor([0.4665], device='cuda:0')
2025-09-01 11:46:30,815 - INFO - largest 10 A_j values: tensor([0.0343, 0.0291, 0.0265, 0.0260, 0.0254, 0.0234, 0.0228, 0.0204, 0.0194,
        0.0188], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 11:46:30,815 - INFO - number of values below 0.01: 136
2025-09-01 11:46:30,816 - INFO - number of values between 0.01 and 0.1: 39
2025-09-01 11:46:30,816 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:46:30,816 - INFO - number of values larger than 0.3: 0
2025-09-01 11:46:30,816 - INFO - train loss is 4.496403249731884e-15
2025-09-01 11:46:30,816 - INFO - impulse response loss is 0.053043182939291
2025-09-01 11:46:30,829 - INFO - Process 0: Completed experiment - student_dim=175, seed=33186
2025-09-01 11:46:40,386 - INFO - delta_l_infinity: tensor([0.9771], device='cuda:1') for w=tensor([-0.6442,  0.2456, -0.4569, -0.9790], device='cuda:1')
2025-09-01 11:46:40,387 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:46:40,387 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:46:40,388 - INFO - initial model: max A_j index: 160
2025-09-01 11:47:09,307 - INFO - final model: max A_j index: 160, max A_j value: 0.035516317933797836, alpha_teacher: tensor([0.6314], device='cuda:1')
2025-09-01 11:47:09,309 - INFO - largest 10 A_j values: tensor([0.0355, 0.0305, 0.0283, 0.0257, 0.0248, 0.0242, 0.0239, 0.0238, 0.0235,
        0.0235], device='cuda:1', grad_fn=<TopkBackward0>)
2025-09-01 11:47:09,310 - INFO - number of values below 0.01: 166
2025-09-01 11:47:09,310 - INFO - number of values between 0.01 and 0.1: 59
2025-09-01 11:47:09,310 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:47:09,311 - INFO - number of values larger than 0.3: 0
2025-09-01 11:47:09,311 - INFO - train loss is 0.0
2025-09-01 11:47:09,311 - INFO - impulse response loss is 0.3031120300292969
2025-09-01 11:47:09,321 - INFO - Process 1: Completed experiment - student_dim=225, seed=85955
2025-09-01 11:47:14,171 - ERROR - G&C failed for student_dim=250, seed=85955: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 1 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 11:47:14,180 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 1 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 11:47:14,180 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:47:14,180 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:47:14,182 - INFO - initial model: max A_j index: 160
2025-09-01 11:47:43,166 - INFO - final model: max A_j index: 160, max A_j value: 0.03525301441550255, alpha_teacher: tensor([0.6314], device='cuda:1')
2025-09-01 11:47:43,168 - INFO - largest 10 A_j values: tensor([0.0353, 0.0281, 0.0254, 0.0246, 0.0245, 0.0239, 0.0236, 0.0235, 0.0233,
        0.0233], device='cuda:1', grad_fn=<TopkBackward0>)
2025-09-01 11:47:43,169 - INFO - number of values below 0.01: 188
2025-09-01 11:47:43,169 - INFO - number of values between 0.01 and 0.1: 62
2025-09-01 11:47:43,169 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:47:43,170 - INFO - number of values larger than 0.3: 0
2025-09-01 11:47:43,170 - INFO - train loss is 1.3579427104559727e-05
2025-09-01 11:47:43,170 - INFO - impulse response loss is 0.3014652729034424
2025-09-01 11:47:43,182 - INFO - Process 1: Completed experiment - student_dim=250, seed=85955
2025-09-01 11:47:48,548 - ERROR - G&C failed for student_dim=275, seed=85955: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 1 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 11:47:48,553 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 1 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 11:47:48,553 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:47:48,554 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:47:48,555 - INFO - initial model: max A_j index: 160
2025-09-01 11:48:17,496 - INFO - final model: max A_j index: 160, max A_j value: 0.035239677876234055, alpha_teacher: tensor([0.6314], device='cuda:1')
2025-09-01 11:48:17,498 - INFO - largest 10 A_j values: tensor([0.0352, 0.0322, 0.0302, 0.0280, 0.0254, 0.0245, 0.0245, 0.0239, 0.0236,
        0.0235], device='cuda:1', grad_fn=<TopkBackward0>)
2025-09-01 11:48:17,499 - INFO - number of values below 0.01: 205
2025-09-01 11:48:17,499 - INFO - number of values between 0.01 and 0.1: 70
2025-09-01 11:48:17,500 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:48:17,500 - INFO - number of values larger than 0.3: 0
2025-09-01 11:48:17,500 - INFO - train loss is 0.0
2025-09-01 11:48:17,500 - INFO - impulse response loss is 0.299801230430603
2025-09-01 11:48:17,514 - INFO - Process 1: Completed experiment - student_dim=275, seed=85955
2025-09-01 11:48:17,516 - INFO - Process 1 completed on GPU 1
2025-09-01 11:48:25,013 - INFO - Process 1 completed
2025-09-01 11:52:56,593 - INFO - delta_l_infinity: tensor([1.2171], device='cuda:0') for w=tensor([ 1.7525, -0.1222, -0.0972, -0.9637], device='cuda:0')
2025-09-01 11:52:56,594 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 11:52:56,595 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 11:52:56,595 - INFO - initial model: max A_j index: 150
2025-09-01 11:53:26,632 - INFO - final model: max A_j index: 150, max A_j value: 0.033495355397462845, alpha_teacher: tensor([0.4665], device='cuda:0')
2025-09-01 11:53:26,635 - INFO - largest 10 A_j values: tensor([0.0335, 0.0283, 0.0257, 0.0252, 0.0246, 0.0227, 0.0220, 0.0218, 0.0196,
        0.0186], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 11:53:26,635 - INFO - number of values below 0.01: 158
2025-09-01 11:53:26,636 - INFO - number of values between 0.01 and 0.1: 42
2025-09-01 11:53:26,636 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 11:53:26,636 - INFO - number of values larger than 0.3: 0
2025-09-01 11:53:26,636 - INFO - train loss is 0.0
2025-09-01 11:53:26,636 - INFO - impulse response loss is 0.05231744796037674
2025-09-01 11:53:26,649 - INFO - Process 0: Completed experiment - student_dim=200, seed=33186
2025-09-01 12:00:39,447 - INFO - delta_l_infinity: tensor([1.2171], device='cuda:0') for w=tensor([ 1.7525, -0.1222, -0.0972, -0.9637], device='cuda:0')
2025-09-01 12:00:39,448 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 12:00:39,448 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 12:00:39,449 - INFO - initial model: max A_j index: 150
2025-09-01 12:01:09,385 - INFO - final model: max A_j index: 150, max A_j value: 0.033254388719797134, alpha_teacher: tensor([0.4665], device='cuda:0')
2025-09-01 12:01:09,387 - INFO - largest 10 A_j values: tensor([0.0333, 0.0281, 0.0255, 0.0249, 0.0244, 0.0224, 0.0218, 0.0216, 0.0194,
        0.0184], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 12:01:09,388 - INFO - number of values below 0.01: 175
2025-09-01 12:01:09,388 - INFO - number of values between 0.01 and 0.1: 50
2025-09-01 12:01:09,388 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 12:01:09,389 - INFO - number of values larger than 0.3: 0
2025-09-01 12:01:09,389 - INFO - train loss is 0.0
2025-09-01 12:01:09,389 - INFO - impulse response loss is 0.05144578218460083
2025-09-01 12:01:09,401 - INFO - Process 0: Completed experiment - student_dim=225, seed=33186
2025-09-01 12:01:09,427 - INFO - Process 0: Starting experiment - student_dim=250, seed=33186
2025-09-01 12:01:14,161 - ERROR - G&C failed for student_dim=250, seed=33186: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 12:01:14,166 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 10.75 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 7.66 GiB memory in use. Of the allocated memory 4.69 GiB is allocated by PyTorch, and 2.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 12:01:14,167 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 12:01:14,167 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 12:01:14,168 - INFO - initial model: max A_j index: 150
2025-09-01 12:01:44,204 - INFO - final model: max A_j index: 150, max A_j value: 0.03347403556108475, alpha_teacher: tensor([0.4665], device='cuda:0')
2025-09-01 12:01:44,206 - INFO - largest 10 A_j values: tensor([0.0335, 0.0283, 0.0257, 0.0252, 0.0246, 0.0226, 0.0220, 0.0218, 0.0196,
        0.0186], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 12:01:44,206 - INFO - number of values below 0.01: 200
2025-09-01 12:01:44,207 - INFO - number of values between 0.01 and 0.1: 50
2025-09-01 12:01:44,207 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 12:01:44,208 - INFO - number of values larger than 0.3: 0
2025-09-01 12:01:44,208 - INFO - train loss is 6.1201044232461754e-15
2025-09-01 12:01:44,208 - INFO - impulse response loss is 0.05079088360071182
2025-09-01 12:01:44,220 - INFO - Process 0: Completed experiment - student_dim=250, seed=33186
2025-09-01 12:01:49,533 - ERROR - G&C failed for student_dim=275, seed=33186: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 0 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-01 12:01:49,536 - ERROR - Traceback (most recent call last):
  File "/home/fodl/noacaspi/ssm_generalize/main.py", line 101, in process_worker
    mean_prior, gnc_gen_loss = train_gnc(seed, student_dim, device, alpha_teacher, w_sequences,
                               ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        args_dict['eps_train'], args_dict['gnc_num_samples'],
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                        batch_size
                                        ^^^^^^^^^^
                                        )
                                        ^
  File "/home/fodl/noacaspi/ssm_generalize/training.py", line 156, in train_gnc
    train_losses, gen_losses = get_losses(students, w, alpha_teacher)
                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fodl/noacaspi/ssm_generalize/losses.py", line 45, in get_losses
    A_pows = torch.cumprod(X, dim=2)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.10 GiB. GPU 0 has a total capacity of 10.75 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 8.40 GiB memory in use. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 3.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-01 12:01:49,537 - INFO - No scheduler or scheduler type 'None', using empty params
2025-09-01 12:01:49,537 - INFO - Calling train_gd with scheduler_params: {}
2025-09-01 12:01:49,538 - INFO - initial model: max A_j index: 150
2025-09-01 12:02:19,479 - INFO - final model: max A_j index: 150, max A_j value: 0.032613396644592285, alpha_teacher: tensor([0.4665], device='cuda:0')
2025-09-01 12:02:19,481 - INFO - largest 10 A_j values: tensor([0.0326, 0.0275, 0.0248, 0.0243, 0.0238, 0.0218, 0.0211, 0.0210, 0.0204,
        0.0187], device='cuda:0', grad_fn=<TopkBackward0>)
2025-09-01 12:02:19,481 - INFO - number of values below 0.01: 222
2025-09-01 12:02:19,482 - INFO - number of values between 0.01 and 0.1: 53
2025-09-01 12:02:19,482 - INFO - number of values between 0.1 and 0.3: 0
2025-09-01 12:02:19,482 - INFO - number of values larger than 0.3: 0
2025-09-01 12:02:19,483 - INFO - train loss is 1.934427906746805e-08
2025-09-01 12:02:19,483 - INFO - impulse response loss is 0.0503406897187233
2025-09-01 12:02:19,496 - INFO - Process 0: Completed experiment - student_dim=275, seed=33186
2025-09-01 12:02:19,496 - INFO - Process 0 completed on GPU 0
2025-09-01 12:02:29,046 - INFO - Process 0 completed
2025-09-01 12:02:30,053 - ERROR - Failed to save checkpoint: division by zero
2025-09-01 12:02:30,054 - INFO - Final checkpoint saved
2025-09-01 12:02:30,114 - INFO - Results saved to results__seq_len=5_seeds=39324-33186-85955-18678-37362_time=20250901_110819.csv
2025-09-01 12:04:57,167 - INFO - Figures saved to plot__seq_len=5_seeds=39324-33186-85955-18678-37362_time=20250901_110819
2025-09-01 12:04:57,170 - INFO - Finished experiments, results saved to results__seq_len=5_seeds=39324-33186-85955-18678-37362_time=20250901_110819.csv, figures saved to plot__seq_len=5_seeds=39324-33186-85955-18678-37362_time=20250901_110819
